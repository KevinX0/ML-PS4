{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"ML-PS2\"\n",
        "author: \"Duoshu Xu\"\n",
        "format: \n",
        "  pcollege_df:\n",
        "    keep-tex: true\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "---\n",
        "\n",
        "\n",
        "**Partnered with Jae Hu**\n"
      ],
      "id": "f756fb6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "id": "6f1843d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 1"
      ],
      "id": "20d5343f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# load the dataset\n",
        "file_path = \"/Users/kevinxu/Documents/GitHub/ML-PS4/Data-College.csv\"\n",
        "college_df = pd.read_csv(file_path)"
      ],
      "id": "1bfcae98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1a"
      ],
      "id": "1eb912b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# drop the first column which is the name of the university\n",
        "college_df = college_df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# convert categorical column 'Private' to numerical\n",
        "college_df['Private'] = college_df['Private'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# define predictors and target variable\n",
        "X = college_df.drop(columns=['Apps'])\n",
        "y = college_df['Apps']\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=37)\n",
        "\n",
        "# scale the predictors\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# convert scaled data back to DataFrame\n",
        "X_train_scaled_college_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
        "X_test_scaled_college_df = pd.DataFrame(X_test_scaled, columns=X.columns)"
      ],
      "id": "273d3193",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1b"
      ],
      "id": "34555f41"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# fit a linear regression model\n",
        "lm = LinearRegression()\n",
        "lm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# predict on the test set\n",
        "y_pred = lm.predict(X_test_scaled)\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "test_mse"
      ],
      "id": "fe6068c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Test MSE: 1222954.0382534422\n",
        "\n",
        "## Question 1c"
      ],
      "id": "0f22f4d1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# define 10-fold cross-validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "\n",
        "# perform PCA on training data\n",
        "pca = PCA()\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# find the optimal number of principal components\n",
        "mse_list = []\n",
        "components_range = range(1, X_train_scaled.shape[1] + 1)\n",
        "\n",
        "for m in components_range:\n",
        "    X_train_pca_m = X_train_pca[:, :m]\n",
        "    lm_pcr = LinearRegression()\n",
        "    mse = -np.mean(cross_val_score(lm_pcr, X_train_pca_m,\n",
        "                   y_train, cv=kf, scoring='neg_mean_squared_error'))\n",
        "    mse_list.append(mse)\n",
        "\n",
        "optimal_m = components_range[np.argmin(mse_list)]\n",
        "\n",
        "# fit PCR model using optimal number of components\n",
        "X_train_pca_opt = X_train_pca[:, :optimal_m]\n",
        "X_test_pca_opt = X_test_pca[:, :optimal_m]\n",
        "lm_pcr_opt = LinearRegression()\n",
        "lm_pcr_opt.fit(X_train_pca_opt, y_train)\n",
        "y_pcr_pred = lm_pcr_opt.predict(X_test_pca_opt)\n",
        "\n",
        "# compute MSE\n",
        "test_mse_pcr = mean_squared_error(y_test, y_pcr_pred)\n",
        "\n",
        "# determine the elbow point using the \"elbow method\"\n",
        "second_derivative = np.diff(mse_list, 2)\n",
        "elbow_m = components_range[np.argmin(second_derivative) + 1]\n",
        "\n",
        "optimal_m, test_mse_pcr, elbow_m"
      ],
      "id": "5824d3a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1d"
      ],
      "id": "203c9b66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# select the optimal number of components for PLS\n",
        "mse_list_pls = []\n",
        "\n",
        "for m in components_range:\n",
        "    pls = PLSRegression(n_components=m)\n",
        "    mse = -np.mean(cross_val_score(pls, X_train_scaled, y_train,\n",
        "                   cv=kf, scoring='neg_mean_squared_error'))\n",
        "    mse_list_pls.append(mse)\n",
        "\n",
        "# minimizing cross-validated error\n",
        "optimal_m_pls = components_range[np.argmin(mse_list_pls)]\n",
        "\n",
        "# fit PLS model\n",
        "pls_opt = PLSRegression(n_components=optimal_m_pls)\n",
        "pls_opt.fit(X_train_scaled, y_train)\n",
        "y_pls_pred = pls_opt.predict(X_test_scaled)\n",
        "\n",
        "# compute MSE\n",
        "test_mse_pls = mean_squared_error(y_test, y_pls_pred)\n",
        "\n",
        "# determine the elbow point\n",
        "second_derivative_pls = np.diff(mse_list_pls, 2)\n",
        "elbow_m_pls = components_range[np.argmin(second_derivative_pls) + 1]\n",
        "\n",
        "optimal_m_pls, test_mse_pls, elbow_m_pls"
      ],
      "id": "03305fb1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1e\n",
        "- The linear regression model (OLS) performed best with the lowest test error (1,222,954.04), indicating that all predictors provided valuable information and there was no significant multicollinearity problem. The PLS model (1,300,987.70) performed better than the PCR (1,487,304.13) because it took into account the information of the response variable during the dimensionality reduction process. Both the PCR and PLS models showed high errors, suggesting that the dimensionality reduction process may have lost some important predictive information. Overall, the OLS model remained the most effective prediction tool, while the PLS model provided a good balance between model complexity and prediction performance.\n",
        "\n",
        "# Question 2\n",
        "## Question 2a"
      ],
      "id": "69ebaa57"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# reate a decision tree graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# adding nodes\n",
        "G.add_node(\"Root\")\n",
        "G.add_node(\"X2 < 1\")\n",
        "G.add_node(\"X1 < 1\")\n",
        "G.add_node(\"X1 < 0\")\n",
        "G.add_node(\"X2 < 0\")\n",
        "G.add_node(\"3\")\n",
        "G.add_node(\"10\")\n",
        "G.add_node(\"0\")\n",
        "G.add_node(\"15\")\n",
        "G.add_node(\"5\")\n",
        "\n",
        "# adding edges\n",
        "G.add_edges_from([\n",
        "    (\"Root\", \"X2 < 1\"), (\"X2 < 1\", \"X1 < 1\"), (\"X1 < 1\",\n",
        "                                               \"X1 < 0\"), (\"X1 < 0\", \"3\"), (\"X1 < 0\", \"X2 < 0\"),\n",
        "    (\"X2 < 0\", \"10\"), (\"X2 < 0\", \"0\"), (\"X1 < 1\", \"15\"), (\"X2 < 1\", \"5\")\n",
        "])\n",
        "\n",
        "# define positions\n",
        "pos = {\n",
        "    \"Root\": (2, 4),\n",
        "    \"X2 < 1\": (2, 3),\n",
        "    \"X1 < 1\": (1, 2),\n",
        "    \"X1 < 0\": (0, 1),\n",
        "    \"X2 < 0\": (1, 1),\n",
        "    \"3\": (-1, 0),\n",
        "    \"10\": (0, 0),\n",
        "    \"0\": (2, 0),\n",
        "    \"15\": (3, 2),\n",
        "    \"5\": (4, 3)\n",
        "}\n",
        "\n",
        "# display the graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"lightblue\",\n",
        "        font_size=10, font_weight=\"bold\", edge_color=\"gray\")\n",
        "plt.title(\"Decision Tree\")\n",
        "plt.show()"
      ],
      "id": "e03aebf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2b"
      ],
      "id": "23248521"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "# set limits and labels\n",
        "ax.set_xlim(-1, 2)\n",
        "ax.set_ylim(0, 3)\n",
        "ax.set_xlabel(\"X1\")\n",
        "ax.set_ylabel(\"X2\")\n",
        "\n",
        "# draw partition lines\n",
        "ax.axhline(y=1, color=\"red\", linestyle=\"dashed\")\n",
        "ax.axhline(y=2, color=\"red\", linestyle=\"dashed\")\n",
        "ax.vlines(x=1, ymin=0, ymax=1, color=\"blue\", linestyle=\"dashed\")\n",
        "ax.vlines(x=0, ymin=1, ymax=2, color=\"blue\", linestyle=\"dashed\")\n",
        "\n",
        "# add region labels (mean values from the tree)\n",
        "ax.text(0, 0.5, \"-1.80\", fontsize=12, ha='center', va='center')\n",
        "ax.text(1.5, 0.5, \"0.63\", fontsize=12, ha='center', va='center')\n",
        "ax.text(-0.5, 1.5, \"-1.06\", fontsize=12, ha='center', va='center')\n",
        "ax.text(1, 1.5, \"0.21\", fontsize=12, ha='center', va='center')\n",
        "ax.text(0.5, 2.5, \"2.49\", fontsize=12, ha='center', va='center')\n",
        "\n",
        "# display the plot\n",
        "plt.title(\"Partition Diagram\")\n",
        "plt.show()"
      ],
      "id": "c5484e51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 3"
      ],
      "id": "0da69b95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "oj_file_path = \"/Users/kevinxu/Documents/GitHub/ML-PS4/Data-OJ.csv\"\n",
        "oj_df = pd.read_csv(oj_file_path)"
      ],
      "id": "10ce45a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3a"
      ],
      "id": "61251d6a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# define variable\n",
        "X = oj_df.drop(columns=['Purchase'])\n",
        "y = oj_df['Purchase']\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=3)"
      ],
      "id": "3f6d6619",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3b"
      ],
      "id": "f6fe5c1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# encode the target variable\n",
        "y_train_encoded = LabelEncoder().fit_transform(y_train)\n",
        "y_test_encoded = LabelEncoder().fit_transform(y_test)\n",
        "\n",
        "# convert categorical predictors to numeric values\n",
        "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
        "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "X_train_encoded, X_test_encoded = X_train_encoded.align(\n",
        "    X_test_encoded, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# fit the full decision tree model\n",
        "tree_clf = DecisionTreeClassifier(random_state=2)\n",
        "tree_clf.fit(X_train_encoded, y_train_encoded)\n",
        "\n",
        "# predict on the training set\n",
        "y_train_pred = tree_clf.predict(X_train_encoded)\n",
        "\n",
        "# compute and display the training error rate\n",
        "train_error_rate = 1 - accuracy_score(y_train_encoded, y_train_pred)\n",
        "train_error_rate"
      ],
      "id": "3e39dc7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3c"
      ],
      "id": "8fc4a8da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# plot the full, unpruned tree\n",
        "plt.figure(figsize=(15, 8))\n",
        "plot_tree(tree_clf, feature_names=X_train_encoded.columns,\n",
        "          class_names=['CH', 'MM'], filled=True, fontsize=6)\n",
        "plt.title(\"Full Unpruned Decision Tree\")\n",
        "plt.show()\n",
        "\n",
        "# fit a pruned decision tree\n",
        "tree_clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=2)\n",
        "tree_clf_pruned.fit(X_train_encoded, y_train_encoded)\n",
        "\n",
        "# plot the pruned tree\n",
        "plt.figure(figsize=(12, 6))\n",
        "plot_tree(tree_clf_pruned, feature_names=X_train_encoded.columns,\n",
        "          class_names=['CH', 'MM'], filled=True, fontsize=10)\n",
        "plt.title(\"Pruned Decision Tree (Max Depth = 3)\")\n",
        "plt.show()\n",
        "\n",
        "# count the number of terminal nodes\n",
        "num_terminal_nodes = sum(tree_clf_pruned.tree_.children_left == -1)\n",
        "\n",
        "num_terminal_nodes"
      ],
      "id": "1c37db6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3d"
      ],
      "id": "9d4e382f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# predict on the test set and compute the confusion matrix\n",
        "y_test_pred = tree_clf.predict(X_test_encoded)\n",
        "conf_matrix = confusion_matrix(y_test_encoded, y_test_pred)\n",
        "\n",
        "# display the test error rate\n",
        "test_error_rate = 1 - accuracy_score(y_test_encoded, y_test_pred)\n",
        "conf_matrix, test_error_rate"
      ],
      "id": "18d9448f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3e"
      ],
      "id": "b72d033f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# perform CCP path to get alpha values\n",
        "path = tree_clf.cost_complexity_pruning_path(X_train_encoded, y_train_encoded)\n",
        "ccp_alphas = path.ccp_alphas[:-1]\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores = []\n",
        "\n",
        "for alpha in ccp_alphas:\n",
        "    pruned_tree = DecisionTreeClassifier(ccp_alpha=alpha, random_state=2)\n",
        "    scores = cross_val_score(pruned_tree, X_train_encoded,\n",
        "                             y_train_encoded, cv=5, scoring='accuracy')\n",
        "    cv_scores.append(1 - np.mean(scores))\n",
        "\n",
        "# find the optimal alpha\n",
        "optimal_alpha = ccp_alphas[np.argmin(cv_scores)]\n",
        "\n",
        "# display the plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ccp_alphas, cv_scores, marker=\"o\", linestyle=\"dashed\",\n",
        "         label=\"Cross-Validated Error Rate\")\n",
        "plt.xlabel(\"Alpha (ccp_alpha)\")\n",
        "plt.ylabel(\"Classification Error Rate\")\n",
        "plt.title(\"Cost Complexity Pruning: Alpha vs. Error Rate\")\n",
        "plt.axvline(optimal_alpha, color=\"red\", linestyle=\"dashed\",\n",
        "            label=f\"Optimal Alpha = {optimal_alpha:.4f}\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "optimal_alpha"
      ],
      "id": "a47cec41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The optimal value of α corresponding to the lowest cross-validated classification error rate is 0.0047. \n",
        "  \n",
        "## Question 3f"
      ],
      "id": "f4d6fd76"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# compute the tree size for each alpha\n",
        "tree_sizes = []\n",
        "\n",
        "for alpha in ccp_alphas:\n",
        "    pruned_tree = DecisionTreeClassifier(ccp_alpha=alpha, random_state=2)\n",
        "    pruned_tree.fit(X_train_encoded, y_train_encoded)\n",
        "    tree_sizes.append(pruned_tree.tree_.node_count)\n",
        "\n",
        "# find the optimal tree size corresponding to the lowest classification error\n",
        "optimal_tree_size = tree_sizes[np.argmin(cv_scores)]\n",
        "\n",
        "# display the plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(tree_sizes, cv_scores, marker=\"o\", linestyle=\"dashed\",\n",
        "         label=\"Cross-Validated Error Rate\")\n",
        "plt.xlabel(\"Tree Size (Number of Nodes)\")\n",
        "plt.ylabel(\"Classification Error Rate\")\n",
        "plt.title(\"Tree Size vs. Classification Error Rate\")\n",
        "plt.axvline(optimal_tree_size, color=\"red\", linestyle=\"dashed\",\n",
        "            label=f\"Optimal Tree Size = {optimal_tree_size}\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "optimal_tree_size"
      ],
      "id": "d1d1064a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The optimal tree size corresponding to the lowest cross-validated classification error rate is 17 nodes.\n",
        "- The α (ccp_alpha) parameter controls pruning by penalizing the complexity of the tree - higher values ​​result in more aggressive pruning and smaller tree size. Smaller α allows for deeper trees, which reduces training error but can easily lead to overfitting and increase test error. Larger α simplifies the tree structure and improves generalization but can lead to underfitting. The ideal α value should balance model complexity and accuracy, preventing overfitting and underfitting, thereby minimizing classification error.\n",
        "\n",
        "## Question 3g"
      ],
      "id": "e2bb2761"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# fit the pruned decision tree using the optimal alpha\n",
        "optimal_pruned_tree = DecisionTreeClassifier(\n",
        "    ccp_alpha=optimal_alpha, random_state=2)\n",
        "optimal_pruned_tree.fit(X_train_encoded, y_train_encoded)\n",
        "\n",
        "# display the pruned tree\n",
        "plt.figure(figsize=(12, 6))\n",
        "plot_tree(optimal_pruned_tree, feature_names=X_train_encoded.columns,\n",
        "          class_names=['CH', 'MM'], filled=True, fontsize=10)\n",
        "plt.title(f\"Optimal Pruned Decision Tree (ccp_alpha = {optimal_alpha:.4f})\")\n",
        "plt.show()"
      ],
      "id": "5d0b3c42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3h"
      ],
      "id": "0f1e0125"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_train_pred_pruned = optimal_pruned_tree.predict(X_train_encoded)\n",
        "train_error_rate_pruned = 1 - \\\n",
        "    accuracy_score(y_train_encoded, y_train_pred_pruned)\n",
        "\n",
        "# compare training error rates\n",
        "train_error_rate, train_error_rate_pruned"
      ],
      "id": "945cc60f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The training error rate for the unpruned tree is 0.67%; the training error rate for the pruned tree is  15.62%.\n",
        "- Pruning reduces the tree’s complexity by removing branches that capture minor variations in the training data, making it less flexible. The unpruned tree overfits by memorizing the training data, resulting in an artificially low training error but a higher test error. ​​\n",
        "\n",
        "## Question 3i"
      ],
      "id": "cb16c273"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_test_pred_pruned = optimal_pruned_tree.predict(X_test_encoded)\n",
        "test_error_rate_pruned = 1 - accuracy_score(y_test_encoded, y_test_pred_pruned)\n",
        "\n",
        "# compare test error rates\n",
        "test_error_rate, test_error_rate_pruned"
      ],
      "id": "e1f73771",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The test error rate for the unpruned tree is 24.3%; the test error rate for the pruned tree is 19.63%.\n",
        "-Pruning removes overly specific splits that capture noise in the training data, preventing overfitting. As a result, the pruned tree generalizes unseen data better, reducing test errors. The unpruned tree overfits the training set, leading to a higher test error due to poor generalization. \n"
      ],
      "id": "f2288a41"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}